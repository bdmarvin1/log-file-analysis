{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bdmarvin1/log-file-analysis/blob/main/Chargebee_Log_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45UFp4D6M1H-"
      },
      "source": [
        "# Enterprise Log File Analysis: Chargebee Case Study\n",
        "This notebook analyzes one year of server log data to diagnose crawl budget waste, identify spider traps, and explain indexing issues.\n",
        "\n",
        "**Objective:** Identify why some pages are \"Crawled - Currently Not Indexed\" and optimize Googlebot's efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoKfEvkgM1IB"
      },
      "source": [
        "## Phase 1: Environment & Dependency Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFP9TCUHM1IC"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import glob\n",
        "import os\n",
        "import socket\n",
        "import gc\n",
        "from datetime import datetime\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from gspread.utils import rowcol_to_a1\n",
        "from google.auth import default\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Constants\n",
        "LOGS_PATH = '/content/drive/MyDrive/Chargebee_Logs/'\n",
        "OUTPUT_SHEET_NAME = 'Chargebee_Audit_Findings'\n",
        "\n",
        "# iPullRank Brand Colors\n",
        "IPR_YELLOW = '#FCD307'\n",
        "IPR_BLUE = '#2A52BE'\n",
        "IPR_BLACK = '#000000'\n",
        "IPR_GREY = '#333333'\n",
        "IPR_LIGHT_GREY = '#CCCCCC'\n",
        "\n",
        "def get_ipr_colors(n, values=None):\n",
        "    if n <= 0: return []\n",
        "    grey_start = 51\n",
        "    grey_end = 204\n",
        "    if values is not None:\n",
        "        v = np.array(values)\n",
        "        if len(v) == 0: return []\n",
        "        sorted_indices = np.argsort(v)[::-1]\n",
        "        colors = ['#CCCCCC'] * len(v)\n",
        "        num_greys = max(1, n - 2)\n",
        "        grey_values = np.linspace(grey_start, grey_end, num_greys)\n",
        "        for i, idx in enumerate(sorted_indices):\n",
        "            if i == 0: colors[idx] = IPR_YELLOW\n",
        "            elif i == 1: colors[idx] = IPR_BLUE\n",
        "            else:\n",
        "                grey_val = int(grey_values[i-2])\n",
        "                colors[idx] = '#%02x%02x%02x' % (grey_val, grey_val, grey_val)\n",
        "        return colors\n",
        "    num_greys = max(1, n - 2)\n",
        "    grey_values = np.linspace(grey_start, grey_end, num_greys)\n",
        "    colors = []\n",
        "    for i in range(n):\n",
        "        if i == 0: colors.append(IPR_YELLOW)\n",
        "        elif i == 1: colors.append(IPR_BLUE)\n",
        "        else:\n",
        "            grey_val = int(grey_values[i-2])\n",
        "            colors.append('#%02x%02x%02x' % (grey_val, grey_val, grey_val))\n",
        "    return colors\n",
        "\n",
        "\n",
        "def calculate_summary_stats(data, global_mean=None, global_std=None):\n",
        "    \"\"\"Calculates summary statistics and SD band counts.\"\"\"\n",
        "    if data is None or len(data) == 0:\n",
        "        return pd.Series(dtype=float)\n",
        "\n",
        "    # Basic Stats\n",
        "    stats = {\n",
        "        'Min': float(data.min()),\n",
        "        'Max': float(data.max()),\n",
        "        'Mean': float(data.mean()),\n",
        "        'Median': float(data.median()),\n",
        "        'Std Dev': float(data.std()),\n",
        "        'Total Count': int(len(data))\n",
        "    }\n",
        "\n",
        "    # Use global stats if provided, otherwise use data's own stats\n",
        "    m = float(global_mean) if global_mean is not None else float(data.mean())\n",
        "    s = float(global_std) if global_std is not None else float(data.std())\n",
        "\n",
        "    if s and s > 0:\n",
        "        stats['Above +2SD'] = int(len(data[data > m + 2*s]))\n",
        "        stats['+1SD to +2SD'] = int(len(data[(data > m + s) & (data <= m + 2*s)]))\n",
        "        stats['-1SD to +1SD'] = int(len(data[(data >= m - s) & (data <= m + s)]))\n",
        "        stats['-2SD to -1SD'] = int(len(data[(data < m - s) & (data >= m - 2*s)]))\n",
        "        stats['Below -2SD'] = int(len(data[data < m - 2*s]))\n",
        "    else:\n",
        "        # If no standard deviation, all items are within 1SD of mean\n",
        "        stats['Above +2SD'] = 0\n",
        "        stats['+1SD to +2SD'] = 0\n",
        "        stats['-1SD to +1SD'] = int(len(data))\n",
        "        stats['-2SD to -1SD'] = 0\n",
        "        stats['Below -2SD'] = 0\n",
        "\n",
        "    return pd.Series(stats)\n",
        "\n",
        "sns.set(style='whitegrid', palette=[IPR_BLUE, IPR_YELLOW, IPR_GREY])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W09BjNgM1ID"
      },
      "source": [
        "## Phase 2: Data Ingestion (Memory Optimized)\n",
        "We process logs in chunks and filter for 'google' immediately to keep the memory footprint small."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJJA7e7eM1ID"
      },
      "outputs": [],
      "source": [
        "def ingest_logs(directory_path, chunk_size=50000):\n",
        "    all_files = glob.glob(os.path.join(directory_path, \"*.csv\"))\n",
        "    if not all_files:\n",
        "        print(f\"No CSV files found in {directory_path}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    googlebot_data = []\n",
        "    rows_kept = 0\n",
        "    total_rows_processed = 0\n",
        "\n",
        "    # Columns we actually need to save memory\n",
        "    use_cols = ['_time', 'useragent', 'uri_path', 'uri_query', 'status',\n",
        "                'bytes_sent', 'clientip', 'method', 'time_taken']\n",
        "\n",
        "    for file in all_files:\n",
        "        print(f\"Processing {file}...\")\n",
        "        try:\n",
        "            # Read in chunks to avoid OOM\n",
        "            for chunk in pd.read_csv(file, usecols=use_cols,\n",
        "                                     chunksize=chunk_size, low_memory=False):\n",
        "                total_rows_processed += len(chunk)\n",
        "                # Filter for Googlebot (case-insensitive)\n",
        "                filtered_chunk = chunk[chunk['useragent'].str.contains(\n",
        "                    \"google\", case=False, na=False)].copy()\n",
        "                rows_kept += len(filtered_chunk)\n",
        "                googlebot_data.append(filtered_chunk)\n",
        "\n",
        "                # Aggressive garbage collection\n",
        "                del chunk\n",
        "                gc.collect()\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "    if not googlebot_data:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = pd.concat(googlebot_data, ignore_index=True)\n",
        "    print(f\"Ingestion complete. Total Googlebot rows: {len(df)}\")\n",
        "    return df\n",
        "\n",
        "df_raw = ingest_logs(LOGS_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv2vlekXM1IE"
      },
      "source": [
        "## Phase 3: Data Cleaning & IP Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo8eTlaGM1IE"
      },
      "outputs": [],
      "source": [
        "def verify_googlebot_ip(ip):\n",
        "    \"\"\"Performs Double-Reverse DNS lookup to verify Googlebot.\"\"\"\n",
        "    try:\n",
        "        host = socket.gethostbyaddr(ip)[0]\n",
        "        if not (host.endswith('.googlebot.com') or \\\n",
        "                host.endswith('.google.com') or \\\n",
        "                host.endswith('.googleusercontent.com')):\n",
        "            return False\n",
        "\n",
        "        addr = socket.gethostbyname(host)\n",
        "        return addr == ip\n",
        "    except (socket.herror, socket.gaierror):\n",
        "        return False\n",
        "\n",
        "def clean_data(df):\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # 1. Timestamp Conversion\n",
        "    df['_time'] = pd.to_datetime(df['_time'])\n",
        "    df.set_index('_time', inplace=True)\n",
        "\n",
        "    # 2. IP Verification (Sample check if dataset is huge, or all unique IPs)\n",
        "    unique_ips = df['clientip'].unique()\n",
        "    print(f\"Verifying {len(unique_ips)} unique IPs...\")\n",
        "    ip_map = {ip: verify_googlebot_ip(ip) for ip in unique_ips}\n",
        "    df['is_verified_bot'] = df['clientip'].map(ip_map)\n",
        "\n",
        "    # 3. File Type Categorization\n",
        "    def get_file_type(path):\n",
        "        if pd.isna(path): return 'Other'\n",
        "        ext = os.path.splitext(path)[1].lower()\n",
        "        if ext in ['', '.html', '.htm']: return 'HTML'\n",
        "        if ext == '.js': return 'JS'\n",
        "        if ext == '.css': return 'CSS'\n",
        "        if ext in ['.jpg', '.jpeg', '.png', '.gif', '.svg',\n",
        "                   '.webp']: return 'Image'\n",
        "        if ext in ['.json', '.xml']: return 'Data'\n",
        "        return 'Other'\n",
        "\n",
        "    df['file_type'] = df['uri_path'].apply(get_file_type)\n",
        "\n",
        "    # 4. URL Normalization\n",
        "    df['full_url'] = df['uri_path'] + df['uri_query'].fillna('')\n",
        "\n",
        "    return df\n",
        "\n",
        "df = clean_data(df_raw)\n",
        "if not df.empty:\n",
        "    print(f\"Verified Bots: {df['is_verified_bot'].sum()} / {len(df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzlxIdAoM1IE"
      },
      "source": [
        "## Phase 4: Core Analysis Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "global_benchmarks"
      },
      "outputs": [],
      "source": [
        "# --- Global Benchmarks for Summary Statistics ---\n",
        "# These are used as reference points for all SD band calculations\n",
        "\n",
        "# 1. Daily Hits Benchmarks (All bots combined)\n",
        "total_daily_hits = df.resample('D').size()\n",
        "global_hit_mean = float(total_daily_hits.mean())\n",
        "global_hit_std = float(total_daily_hits.std())\n",
        "\n",
        "# 2. Weekly Hits Benchmarks (Only full weeks)\n",
        "days_per_week = df.index.to_series().resample('W').apply(lambda x:\n",
        "                                                         x.dt.date.nunique())\n",
        "full_weeks = days_per_week[days_per_week == 7].index\n",
        "total_weekly_hits = df.resample('W').size().loc[full_weeks]\n",
        "global_weekly_hit_mean = float(total_weekly_hits.mean())\n",
        "global_weekly_hit_std = float(total_weekly_hits.std())\n",
        "\n",
        "# 3. Site-Wide Performance Benchmarks (All file types)\n",
        "global_latency_mean = float(df['time_taken'].mean())\n",
        "global_latency_std = float(df['time_taken'].std())\n",
        "global_bandwidth_mean = float(df['bytes_sent'].mean())\n",
        "global_bandwidth_std = float(df['bytes_sent'].std())\n",
        "\n",
        "print(f\"\"\"Global Daily Hit Mean: {global_hit_mean:.2f}\n",
        "      (SD: {global_hit_std:.2f})\"\"\")\n",
        "print(f\"\"\"Global Site Latency Mean: {global_latency_mean:.2f}ms\n",
        "      (SD: {global_latency_std:.2f})\"\"\")\n",
        "print(f\"\"\"Global Site Bandwidth Mean: {global_bandwidth_mean:.2f} bytes\n",
        "      (SD: {global_bandwidth_std:.2f})\"\"\")\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4TgqL_MM1IF"
      },
      "source": [
        "### Module A: Crawl Volume & Frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "total_hits_analysis"
      },
      "outputs": [],
      "source": [
        "# --- Total Hits Analysis (Daily & Weekly) ---\n",
        "\n",
        "# 1. Daily Total Hits\n",
        "plt.figure(figsize=(14, 6))\n",
        "total_daily_hits.plot(color=IPR_BLUE, linewidth=2)\n",
        "plt.title('Total Googlebot Hits per Day', color=IPR_GREY, fontsize=16,\n",
        "          fontweight=\"bold\")\n",
        "plt.ylabel('Hits')\n",
        "plt.show()\n",
        "\n",
        "daily_hits_summary = calculate_summary_stats(total_daily_hits,\n",
        "                                             global_hit_mean, global_hit_std)\n",
        "daily_hits_summary_df = pd.DataFrame(daily_hits_summary).T\n",
        "daily_hits_summary_df.index = ['Total Daily Hits']\n",
        "\n",
        "# 2. Weekly Total Hits\n",
        "plt.figure(figsize=(14, 6))\n",
        "total_weekly_hits.plot(kind='bar', color=IPR_BLUE)\n",
        "plt.title('Total Googlebot Hits per Week', color=IPR_GREY,\n",
        "          fontsize=16, fontweight=\"bold\")\n",
        "plt.ylabel('Hits')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "weekly_hits_summary = calculate_summary_stats(total_weekly_hits,\n",
        "                                              global_weekly_hit_mean,\n",
        "                                              global_weekly_hit_std)\n",
        "weekly_hits_summary_df = pd.DataFrame(weekly_hits_summary).T\n",
        "weekly_hits_summary_df.index = ['Total Weekly Hits']\n",
        "\n",
        "print(\"Daily Hits Summary Stats:\")\n",
        "display(daily_hits_summary_df)\n",
        "print(\"\\nWeekly Hits Summary Stats:\")\n",
        "display(weekly_hits_summary_df)\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dC7qywJ-M1IF"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def get_bot_type(ua):\n",
        "    ua = str(ua)\n",
        "    if re.search(r'Googlebot-Image', ua, re.I): return 'Image'\n",
        "    if re.search(r'Googlebot-Video', ua, re.I): return 'Video'\n",
        "    if re.search(r'Googlebot-News', ua, re.I): return 'News'\n",
        "    if re.search(r'Storebot-Google', ua, re.I): return 'StoreBot'\n",
        "    if re.search(r'AdsBot-Google', ua, re.I): return 'AdsBot'\n",
        "    if re.search(r'Mediapartners-Google', ua, re.I): return 'AdSense'\n",
        "    if re.search(r'Google-InspectionTool', ua, re.I): return 'Inspection Tool'\n",
        "    if re.search(r'Googlebot', ua, re.I):\n",
        "        if re.search(r'Mobile|Android|iPhone', ua, re.I): return 'Smartphone'\n",
        "        return 'Desktop'\n",
        "    return 'Other Google/Unknown'\n",
        "\n",
        "df['bot_type'] = df['useragent'].apply(get_bot_type)\n",
        "bot_counts = df['bot_type'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = get_ipr_colors(len(bot_counts), values=bot_counts.values)\n",
        "sns.barplot(x=bot_counts.index, y=bot_counts.values, palette=colors)\n",
        "plt.title('Total Hits per Googlebot Type', color=IPR_GREY, fontsize=16,\n",
        "          fontweight=\"bold\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "top_5_bots = bot_counts.head(5).index.tolist()\n",
        "bot_time_series = df[df['bot_type'].isin(top_5_bots)].groupby(\n",
        "    ['bot_type']).resample('D').size().unstack(0).fillna(0)\n",
        "plt.figure(figsize=(14, 7))\n",
        "line_colors = get_ipr_colors(len(top_5_bots),\n",
        "                             values=[bot_counts[b] for b in top_5_bots])\n",
        "bot_time_series[top_5_bots].plot(ax=plt.gca(), color=line_colors, linewidth=2)\n",
        "plt.title('Top 5 Googlebot Types Over Time',\n",
        "          color=IPR_GREY, fontsize=16, fontweight=\"bold\")\n",
        "plt.show()\n",
        "# Calculate global benchmarks for daily hits\n",
        "\n",
        "# Calculate summary statistics for Top Bot Types\n",
        "bot_summary_stats = {}\n",
        "for bot_type in bot_time_series.columns:\n",
        "    bot_summary_stats[bot_type] = \\\n",
        "    calculate_summary_stats(bot_time_series[bot_type],\n",
        "                            global_hit_mean, global_hit_std)\n",
        "bot_summary_df = pd.DataFrame(bot_summary_stats).T\n",
        "\n",
        "print(\"Bot Type Summary Statistics (Relative to Global Daily Average):\")\n",
        "display(bot_summary_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "robots-txt-header"
      },
      "source": [
        "### Module A.2: robots.txt Analysis\n",
        "This module tracks how many times Googlebot requested the robots.txt file and what status codes were returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "robots-txt-code"
      },
      "outputs": [],
      "source": [
        "# --- robots.txt Analysis ---\n",
        "print(\"Analyzing robots.txt hits...\")\n",
        "robots_df = df[df['uri_path'].str.contains('robots.txt', na=False, case=False)]\n",
        "robots_summary_df = robots_df.groupby(['uri_path', 'status']).size().reset_index(name='hits')\n",
        "\n",
        "display(robots_summary_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH9kUlFGM1IF"
      },
      "source": [
        "### Module B: Status Code Health"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cache_lifecycle_analysis"
      },
      "outputs": [],
      "source": [
        "# --- Cache Lifecycle & New Page Analysis ---\n",
        "\n",
        "print(\"Analyzing page-level cache transitions...\")\n",
        "\n",
        "# Sort by time to analyze sequence\n",
        "df_sorted = df.sort_index()\n",
        "\n",
        "# 1. Average 200s before first 304\n",
        "def calc_200s_before_304(group):\n",
        "    status_list = group['status'].tolist()\n",
        "    try:\n",
        "        first_304_idx = status_list.index(304)\n",
        "        return status_list[:first_304_idx].count(200)\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "page_transitions = df_sorted.groupby('full_url').apply(calc_200s_before_304)\n",
        "pages_with_304 = page_transitions.dropna()\n",
        "avg_200s_before_304 = pages_with_304.mean() if not pages_with_304.empty else 0\n",
        "\n",
        "# 2. \"New Page\" Analysis\n",
        "first_hits = df_sorted.groupby('full_url').head(1)\n",
        "new_page_urls = first_hits[first_hits['status'] == 200]['full_url'].unique()\n",
        "\n",
        "new_page_data = df_sorted[df_sorted['full_url'].isin(new_page_urls)]\n",
        "new_page_ever_304 = new_page_data.groupby('full_url')['status']. \\\n",
        "        apply(lambda x: 304 in x.values)\n",
        "pct_new_pages_caching = (new_page_ever_304.sum() / \\\n",
        "                         len(new_page_ever_304) * 100) if \\\n",
        "                         len(new_page_ever_304) > 0 else 0\n",
        "\n",
        "# 3. Potential Hits Eliminated (Theoretical Savings)\n",
        "# Logic: Max(0, Count of 200 OK responses - 2) per URL\n",
        "url_200_counts = df[df['status'] == 200]['full_url'].value_counts()\n",
        "potential_eliminated_hits = url_200_counts.apply(lambda x: max(0, x - 2)).sum()\n",
        "pct_total_hits_savable = (potential_eliminated_hits / len(df) * 100)\n",
        "\n",
        "# Summary Table\n",
        "cache_summary_df = pd.DataFrame({\n",
        "    'Metric': [\n",
        "        'Avg 200s before first 304',\n",
        "        'Total Pages analyzed for 304 transition',\n",
        "        'Total \"New Pages\" (Start with 200)',\n",
        "        'New Pages that eventually got a 304',\n",
        "        '% of New Pages that cached',\n",
        "        'Potential Hits Eliminated (Automated 304 Implementation)',\n",
        "        '% of Total Hits theoretically eliminable'\n",
        "    ],\n",
        "    'Value': [\n",
        "        avg_200s_before_304,\n",
        "        len(pages_with_304),\n",
        "        len(new_page_ever_304),\n",
        "        new_page_ever_304.sum(),\n",
        "        pct_new_pages_caching,\n",
        "        potential_eliminated_hits,\n",
        "        pct_total_hits_savable\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(f\"Average 200s before 304: {avg_200s_before_304:.2f}\")\n",
        "print(f\"Percentage of new pages that eventually show a 304: {pct_new_pages_caching:.2f}%\")\n",
        "print(f\"Potential hits eliminated: {potential_eliminated_hits:,} ({pct_total_hits_savable:.2f}% of total crawl volume)\")\n",
        "display(cache_summary_df)\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLDEZLKaM1IF"
      },
      "outputs": [],
      "source": [
        "status_summary = df.groupby(['status', 'file_type']).size().unstack(fill_value=0)\n",
        "status_totals = df['status'].value_counts()\n",
        "\n",
        "status_time_series = df.groupby(['status']).resample('D').size().unstack(0).fillna(0)\n",
        "plt.figure(figsize=(14, 7))\n",
        "st_colors = get_ipr_colors(len(status_totals), values=status_totals.values)\n",
        "st_map = dict(zip(status_totals.index, st_colors))\n",
        "line_colors = [st_map.get(s, '#CCCCCC') for s in status_time_series.columns]\n",
        "status_time_series.plot(ax=plt.gca(), color=line_colors, linewidth=2)\n",
        "plt.title('Status Code Frequency Over Time', color=IPR_GREY, fontsize=16, fontweight=\"bold\")\n",
        "plt.show()\n",
        "# Calculate summary statistics for Status Codes\n",
        "status_code_summary_stats = {}\n",
        "for status_code in status_time_series.columns:\n",
        "    status_code_summary_stats[status_code] = calculate_summary_stats(status_time_series[status_code], global_hit_mean, global_hit_std)\n",
        "status_code_summary_df = pd.DataFrame(status_code_summary_stats).T\n",
        "\n",
        "print(\"Status Code Summary Statistics (Relative to Global Daily Average):\")\n",
        "display(status_code_summary_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLcSEIWRM1IG"
      },
      "source": [
        "### Module C: Spider Trap Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38aFaruhM1IG"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import parse_qs\n",
        "\n",
        "def analyze_spider_traps(df, threshold=50):\n",
        "    traps = df.groupby('uri_path').agg(\n",
        "        unique_variations=('full_url', 'nunique'),\n",
        "        total_hits=('full_url', 'count'),\n",
        "        sample_urls=('full_url', lambda x: list(x.unique()[:3]))\n",
        "    ).reset_index()\n",
        "    top_traps_summary = traps[traps['unique_variations'] > threshold].sort_values(by='unique_variations', ascending=False)\n",
        "    trap_details = []\n",
        "    filter_keywords = ['sort', 'filter', 'price', 'color', 'size', 'category', 'page', 'offset', 'limit', 'order']\n",
        "    doc_extensions = ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.csv']\n",
        "    for _, row in top_traps_summary.iterrows():\n",
        "        path = row['uri_path']\n",
        "        trap_data = df[df['uri_path'] == path]\n",
        "\n",
        "        # Calculate Avg Latency and Total Bytes for each trap\n",
        "        avg_latency = trap_data['time_taken'].mean()\n",
        "        total_bytes = trap_data['bytes_sent'].sum()\n",
        "\n",
        "        param_counts = {}; combo_counts = {}\n",
        "        for query in trap_data['uri_query'].dropna():\n",
        "            params = parse_qs(query.lstrip('?'))\n",
        "            for p in params.keys(): param_counts[p] = param_counts.get(p, 0) + 1\n",
        "            combo = '+'.join(sorted(params.keys()))\n",
        "            if combo: combo_counts[combo] = combo_counts.get(combo, 0) + 1\n",
        "        is_doc = any(path.lower().endswith(ext) for ext in doc_extensions)\n",
        "        found_filters = [p for p in param_counts.keys() if any(k in p.lower() for k in filter_keywords)]\n",
        "        trap_details.append({\n",
        "            'uri_path': path,\n",
        "            'Variations': row['unique_variations'],\n",
        "            'Hits': row['total_hits'],\n",
        "            'Avg Latency': avg_latency, # Added Avg Latency\n",
        "            'Total Bytes': total_bytes, # Added Total Bytes\n",
        "            'IsDoc': 'Yes' if is_doc else 'No',\n",
        "            'FileTypes': trap_data['file_type'].value_counts().to_dict(),\n",
        "            'Top Bloat Params': sorted(param_counts.items(), key=lambda x: x[1], reverse=True)[:3],\n",
        "            'Top Combos': sorted(combo_counts.items(), key=lambda x: x[1], reverse=True)[:3],\n",
        "            'FilterParams': found_filters,\n",
        "            'URL Samples': row['sample_urls']\n",
        "        })\n",
        "    return pd.DataFrame(trap_details)\n",
        "\n",
        "trap_details_df = analyze_spider_traps(df)\n",
        "display(trap_details_df.head(10))\n",
        "# Summary statistics for the Spider Trap Analysis table\n",
        "trap_summary_df = pd.DataFrame({\n",
        "    'Variations': calculate_summary_stats(trap_details_df['Variations']),\n",
        "    'Avg Latency': calculate_summary_stats(trap_details_df['Avg Latency']),\n",
        "    'Total Bytes': calculate_summary_stats(trap_details_df['Total Bytes'])\n",
        "}).T\n",
        "print(\"Spider Trap Summary Statistics:\")\n",
        "display(trap_summary_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhjoxiuwM1IG"
      },
      "source": [
        "### Module D: Rendering Budget Impact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIIex0MUM1IG"
      },
      "outputs": [],
      "source": [
        "budget_impact = df.groupby('file_type')['bytes_sent'].sum().sort_values(ascending=False).reset_index()\n",
        "budget_impact['GB'] = budget_impact['bytes_sent'] / (1024**3)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "# Use rank-based coloring\n",
        "colors_pie = get_ipr_colors(len(budget_impact), values=budget_impact['bytes_sent'].values)\n",
        "plt.pie(budget_impact['bytes_sent'], labels=budget_impact['file_type'], autopct='%1.1f%%', colors=colors_pie, startangle=140)\n",
        "plt.title(\"Crawl Budget by Content Type (Bandwidth Usage)\", color=IPR_GREY, fontsize=16, fontweight='bold')\n",
        "plt.show()\n",
        "# Detailed summary statistics for bandwidth usage per file type\n",
        "budget_summary_list = []\n",
        "for ftype in df['file_type'].unique():\n",
        "    ftype_data = df[df['file_type'] == ftype]['bytes_sent']\n",
        "    stats = calculate_summary_stats(ftype_data, global_bandwidth_mean, global_bandwidth_std)\n",
        "    stats['file_type'] = ftype\n",
        "    budget_summary_list.append(stats)\n",
        "budget_summary_df = pd.DataFrame(budget_summary_list).set_index('file_type')\n",
        "print(\"Rendering Budget Summary Statistics (Bandwidth per Request vs Site Average):\")\n",
        "display(budget_summary_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "largest-requests-header"
      },
      "source": [
        "### Module D.2: Largest Requests Breakdown\n",
        "This module identifies the resources consuming the most bandwidth and provides a breakdown of their status codes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "largest-requests-code"
      },
      "outputs": [],
      "source": [
        "# --- Largest Requests Breakdown ---\n",
        "print(\"Analyzing largest requests...\")\n",
        "\n",
        "largest_requests_df = df.groupby('full_url').agg(\n",
        "    avg_size_bytes=('bytes_sent', 'mean'),\n",
        "    total_requests=('full_url', 'count'),\n",
        "    total_data_mb=('bytes_sent', lambda x: x.sum() / (1024 * 1024))\n",
        ").reset_index()\n",
        "\n",
        "# Status code breakdown\n",
        "status_pivot = df.pivot_table(index='full_url', columns='status', aggfunc='size', fill_value=0)\n",
        "largest_requests_df = largest_requests_df.merge(status_pivot, on='full_url', how='left')\n",
        "\n",
        "# Sort by total data transmitted and take top 50\n",
        "largest_requests_df = largest_requests_df.sort_values('total_data_mb', ascending=False).head(50)\n",
        "\n",
        "display(largest_requests_df.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEaDDNGdM1IG"
      },
      "source": [
        "## Phase 5: Additional Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Module E: Advanced Directory & Performance Analysis"
      ],
      "metadata": {
        "id": "T-YGh0BIhBFC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIY0M1P8IjWy"
      },
      "outputs": [],
      "source": [
        "def get_directory(path, depth=1, folders_set=None):\n",
        "    if pd.isna(path) or path == '/': return '/'\n",
        "    parts = [p for p in path.split('/') if p]\n",
        "    if not parts: return '/'\n",
        "\n",
        "    # If no folders_set is provided, we can't do smart detection, fall back to simple depth\n",
        "    if folders_set is None:\n",
        "        d = depth if depth is not None else len(parts)\n",
        "        return '/' + '/'.join(parts[:d])\n",
        "\n",
        "    # Try to find the longest prefix that is a confirmed folder, up to the desired depth\n",
        "    max_d = depth if depth is not None else len(parts)\n",
        "    for i in range(min(len(parts), max_d), 0, -1):\n",
        "        prefix = '/' + '/'.join(parts[:i])\n",
        "        if prefix in folders_set:\n",
        "            return prefix\n",
        "\n",
        "    return '/'\n",
        "\n",
        "# Identify all confirmed folders (paths that contain other resources)\n",
        "unique_paths = df['uri_path'].unique()\n",
        "confirmed_folders = set()\n",
        "for path in unique_paths:\n",
        "    if pd.isna(path): continue\n",
        "    parts = [p for p in path.split('/') if p]\n",
        "    # Every prefix of a path is a folder that contains the subsequent part\n",
        "    for i in range(1, len(parts)):\n",
        "        confirmed_folders.add('/' + '/'.join(parts[:i]))\n",
        "\n",
        "# Use the global folders set to accurately identify directories limiting depth to 1 level\n",
        "df['directory'] = df['uri_path'].apply(lambda x: get_directory(x, depth=1, folders_set=confirmed_folders))\n",
        "\n",
        "# Filter to include only HTML pages for directory crawl analysis (exclude static assets as requested)\n",
        "df_dir_analysis = df[df['file_type'] == 'HTML'].copy()\n",
        "\n",
        "# 1. Top 10 Most Crawled Directories\n",
        "top_dirs = df_dir_analysis['directory'].value_counts().head(10)\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = get_ipr_colors(len(top_dirs), values=top_dirs.values)\n",
        "sns.barplot(x=top_dirs.values, y=top_dirs.index, palette=colors)\n",
        "plt.title('Top 10 Most Crawled Directories', color=IPR_GREY, fontsize=16, fontweight=\"bold\")\n",
        "plt.show()\n",
        "\n",
        "# 2. Bottom Least Crawled Directories (Hits = 1 or Bottom 10)\n",
        "dir_counts = df_dir_analysis['directory'].value_counts()\n",
        "bottom_dirs = dir_counts[dir_counts == 1]\n",
        "if len(bottom_dirs) < 10: bottom_dirs = dir_counts.tail(10)\n",
        "\n",
        "bottom_dirs_vis = bottom_dirs.tail(20) # Limit visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = get_ipr_colors(len(bottom_dirs_vis), values=bottom_dirs_vis.values)\n",
        "sns.barplot(x=bottom_dirs_vis.values, y=bottom_dirs_vis.index, palette=colors)\n",
        "plt.title('Least Crawled Directories (Total Hits)', color=IPR_GREY, fontsize=16, fontweight=\"bold\")\n",
        "plt.show()\n",
        "\n",
        "# 3. Least Crawled Relative to Size (Hits / Unique URLs)\n",
        "dir_stats = df_dir_analysis.groupby('directory').agg(\n",
        "    hits=('full_url', 'count'),\n",
        "    unique_urls=('full_url', 'nunique')\n",
        ").reset_index()\n",
        "dir_stats['hits_per_page'] = dir_stats['hits'] / dir_stats['unique_urls']\n",
        "bottom_relative = dir_stats[dir_stats['hits_per_page'] < 10]\n",
        "if len(bottom_relative) < 10: bottom_relative = dir_stats.sort_values('hits_per_page').head(10)\n",
        "\n",
        "bottom_rel_vis = bottom_relative.head(20) # Limit visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = get_ipr_colors(len(bottom_rel_vis), values=bottom_rel_vis['hits_per_page'].values)\n",
        "sns.barplot(x=bottom_rel_vis['hits_per_page'].values, y=bottom_rel_vis['directory'].values, palette=colors)\n",
        "plt.title('Bottom Directories by Crawl Frequency (Hits/Unique URL)', color=IPR_GREY, fontsize=16, fontweight=\"bold\")\n",
        "plt.show()\n",
        "\n",
        "# 4. Performance: Top 10 Directories by Average Latency (ms)\n",
        "avg_latency = df_dir_analysis.groupby('directory')['time_taken'].mean().sort_values(ascending=False).head(10)\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = get_ipr_colors(len(avg_latency), values=avg_latency.values)\n",
        "sns.barplot(x=avg_latency.values, y=avg_latency.index, palette=colors)\n",
        "plt.title('Top 10 Directories by Average Latency (ms)', color=IPR_GREY, fontsize=16, fontweight=\"bold\")\n",
        "plt.show()\n",
        "\n",
        "# 5. Bandwidth: Top 10 Directories by Total Bandwidth (MB)\n",
        "total_bandwidth = (df_dir_analysis.groupby('directory')['bytes_sent'].sum() / (1024 * 1024)).sort_values(ascending=False).head(10)\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = get_ipr_colors(len(total_bandwidth), values=total_bandwidth.values)\n",
        "sns.barplot(x=total_bandwidth.values, y=total_bandwidth.index, palette=colors)\n",
        "plt.title('Top 10 Directories by Total Bandwidth (MB)', color=IPR_GREY, fontsize=16, fontweight=\"bold\")\n",
        "plt.show()\n",
        "\n",
        "# 6. Errors: Top 10 Directories by Error Count (4xx/5xx)\n",
        "df_dir_analysis['is_error'] = df_dir_analysis['status'].apply(lambda x: x >= 400)\n",
        "error_counts = df_dir_analysis.groupby('directory')['is_error'].sum().sort_values(ascending=False).head(10)\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = get_ipr_colors(len(error_counts), values=error_counts.values)\n",
        "sns.barplot(x=error_counts.values, y=error_counts.index, palette=colors)\n",
        "plt.title('Top 10 Directories by Error Count (4xx/5xx)', color=IPR_GREY, fontsize=16, fontweight=\"bold\")\n",
        "plt.show()\n",
        "\n",
        "top_10_dir_list = top_dirs.index.tolist()\n",
        "dir_time_series = df_dir_analysis[df_dir_analysis['directory'].isin(top_10_dir_list)].groupby(['directory']).resample('D').size().unstack(0).fillna(0)\n",
        "plt.figure(figsize=(14, 7))\n",
        "line_colors = get_ipr_colors(len(top_10_dir_list), values=[top_dirs[d] for d in top_10_dir_list])\n",
        "dir_time_series[top_10_dir_list].plot(ax=plt.gca(), color=line_colors, linewidth=2)\n",
        "plt.title('Top 10 Crawled Folders Over Time', color=IPR_GREY, fontsize=16, fontweight=\"bold\")\n",
        "plt.show()\n",
        "# Calculate global benchmarks for performance (HTML only)\n",
        "\n",
        "# Calculate summary statistics for ALL directories\n",
        "dir_perf_list = []\n",
        "for directory in df_dir_analysis['directory'].unique():\n",
        "    dir_data = df_dir_analysis[df_dir_analysis['directory'] == directory]\n",
        "    l_stats = calculate_summary_stats(dir_data['time_taken'], global_latency_mean, global_latency_std)\n",
        "    l_stats.index = ['Latency_' + str(x) for x in l_stats.index]\n",
        "    b_stats = calculate_summary_stats(dir_data['bytes_sent'], global_bandwidth_mean, global_bandwidth_std)\n",
        "    b_stats.index = ['Bandwidth_' + str(x) for x in b_stats.index]\n",
        "    combined = pd.concat([l_stats, b_stats])\n",
        "    combined['directory'] = directory\n",
        "    dir_perf_list.append(combined)\n",
        "\n",
        "dir_performance_summary_df = pd.DataFrame(dir_perf_list).set_index('directory')\n",
        "print(\"Directory Performance Summary (Latency & Bandwidth vs Site Average):\")\n",
        "display(dir_performance_summary_df.head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_status = df_dir_analysis[df_dir_analysis['directory'].isin(top_dirs.index)].groupby(['directory', 'status']).size().unstack(fill_value=0)\n",
        "plt.figure(figsize=(12, 6))\n",
        "global_status_counts = df_dir_analysis['status'].value_counts()\n",
        "status_colors = get_ipr_colors(len(global_status_counts), values=global_status_counts.values)\n",
        "status_color_map = dict(zip(global_status_counts.index, status_colors))\n",
        "plot_colors = [status_color_map.get(s, '#CCCCCC') for s in folder_status.columns]\n",
        "folder_status.plot(kind='bar', stacked=True, ax=plt.gca(), color=plot_colors)\n",
        "plt.title('Status Code Distribution per Top Folder', color=IPR_GREY, fontsize=16, fontweight=\"bold\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DDQsNjVhvmGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HEDRpyuM1IH"
      },
      "outputs": [],
      "source": [
        "def export_all_to_sheets(dataset_list):\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default()\n",
        "    gc_sheet = gspread.authorize(creds)\n",
        "\n",
        "    try:\n",
        "        sh = gc_sheet.open(OUTPUT_SHEET_NAME)\n",
        "    except gspread.SpreadsheetNotFound:\n",
        "        sh = gc_sheet.create(OUTPUT_SHEET_NAME)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%m/%d %H:%M\")\n",
        "\n",
        "    # Create a new worksheet for this consolidated run\n",
        "    # Increased columns to 100 for horizontal arrangement\n",
        "    ws = sh.add_worksheet(title=f\"Analysis {timestamp}\", rows=2000, cols=200)\n",
        "\n",
        "    curr_col = 1\n",
        "    for title, df in dataset_list:\n",
        "        if df is None:\n",
        "            continue\n",
        "        if isinstance(df, pd.Series):\n",
        "            df = df.reset_index()\n",
        "        if df.empty:\n",
        "            continue\n",
        "\n",
        "        # Write Title at Row 1\n",
        "        title_cell = rowcol_to_a1(1, curr_col)\n",
        "        ws.update(range_name=title_cell, values=[[title]])\n",
        "\n",
        "        # Prepare Header and Data\n",
        "        if isinstance(df.columns, pd.MultiIndex):\n",
        "            df.columns = ['_'.join([str(c) for c in col]).strip() for col in df.columns.values]\n",
        "\n",
        "        # Convert datetimes to simple strings (m/d/yy) for better chart display\n",
        "        # Also convert any lists or dictionaries to string representations\n",
        "        for col in df.columns:\n",
        "            if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
        "                df[col] = df[col].dt.strftime('%m/%d/%y')\n",
        "            # Check if any element in the column is a list or dictionary\n",
        "            # and convert the entire column to string type if so.\n",
        "            elif df[col].apply(lambda x: isinstance(x, (list, dict))).any():\n",
        "                df[col] = df[col].astype(str)\n",
        "\n",
        "\n",
        "        header = [f\"'{col}\" for col in df.columns.tolist()]\n",
        "        # Use empty strings for NaNs to ensure JSON compliance and preserve empty cells in sheets\n",
        "        data = df.fillna('').values.tolist()\n",
        "\n",
        "        # Write Header and Data starting at Row 2\n",
        "        data_cell = rowcol_to_a1(2, curr_col)\n",
        "        ws.update(range_name=data_cell, values=[header] + data, value_input_option='USER_ENTERED')\n",
        "\n",
        "        # Move to next starting column: width of current DF + 1 empty column\n",
        "        curr_col += len(df.columns) + 1\n",
        "\n",
        "    print(f\"All datasets exported successfully! URL: {sh.url}\")\n",
        "\n",
        "# Construct the list of all datasets for export\n",
        "export_list = [\n",
        "    (\"Total Hits (Daily)\", total_daily_hits.reset_index() if 'total_daily_hits' in locals() else None),\n",
        "    (\"Total Hits (Daily) Summary Stats\", daily_hits_summary_df.reset_index() if 'daily_hits_summary_df' in locals() else None),\n",
        "    (\"Total Hits (Weekly)\", total_weekly_hits.reset_index() if 'total_weekly_hits' in locals() else None),\n",
        "    (\"Total Hits (Weekly) Summary Stats\", weekly_hits_summary_df.reset_index() if 'weekly_hits_summary_df' in locals() else None),\n",
        "    (\"Bot Type Breakdown\", bot_counts if 'bot_counts' in locals() else None),\n",
        "    (\"Bot Type Trends (Daily)\", bot_time_series.reset_index() if 'bot_time_series' in locals() else None),\n",
        "    (\"Bot Type Summary Stats\", bot_summary_df.reset_index() if 'bot_summary_df' in locals() else None),\n",
        "    (\"robots.txt Analysis\", robots_summary_df if 'robots_summary_df' in locals() else None),\n",
        "    (\"Status Code Summary\", status_summary.reset_index() if 'status_summary' in locals() else None),\n",
        "    (\"Status Code Trends (Daily)\", status_time_series.reset_index() if 'status_time_series' in locals() else None),\n",
        "    (\"Status Code Summary Stats\", status_code_summary_df.reset_index() if 'status_code_summary_df' in locals() else None),\n",
        "    (\"Cache & New Page Analysis\", cache_summary_df.reset_index() if 'cache_summary_df' in locals() else None),\n",
        "    (\"Spider Trap Analysis\", trap_details_df if 'trap_details_df' in locals() else None),\n",
        "    (\"Spider Trap Summary Stats\", trap_summary_df.reset_index() if 'trap_summary_df' in locals() else None),\n",
        "    (\"Rendering Budget Impact\", budget_impact if 'budget_impact' in locals() else None),\n",
        "    (\"Rendering Budget Summary Stats\", budget_summary_df.reset_index() if 'budget_summary_df' in locals() else None),\n",
        "    (\"Largest Requests Breakdown\", largest_requests_df if 'largest_requests_df' in locals() else None),\n",
        "    (\"Top Most Crawled Directories\", top_dirs if 'top_dirs' in locals() else None),\n",
        "    (\"Bottom Directories (Least Crawled)\", bottom_dirs if 'bottom_dirs' in locals() else None),\n",
        "    (\"Bottom Directories (Crawl Frequency)\", bottom_relative if 'bottom_relative' in locals() else None),\n",
        "    (\"Top 10 Directories (Latency)\", avg_latency if 'avg_latency' in locals() else None),\n",
        "    (\"Top 10 Directories (Bandwidth)\", total_bandwidth if 'total_bandwidth' in locals() else None),\n",
        "    (\"Top 10 Directories (Errors)\", error_counts if 'error_counts' in locals() else None),\n",
        "    (\"Folder Trends (Daily)\", dir_time_series.reset_index() if 'dir_time_series' in locals() else None),\n",
        "    (\"Folder Performance Summary\", dir_performance_summary_df.reset_index() if 'dir_performance_summary_df' in locals() else None),\n",
        "    (\"Status Code Distribution per Folder\", folder_status.reset_index() if 'folder_status' in locals() else None)\n",
        "]\n",
        "\n",
        "export_all_to_sheets(export_list)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}