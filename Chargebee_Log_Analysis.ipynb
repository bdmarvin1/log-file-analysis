{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enterprise Log File Analysis: Chargebee Case Study\n",
    "This notebook analyzes one year of server log data to diagnose crawl budget waste, identify spider traps, and explain indexing issues.\n",
    "\n",
    "**Objective:** Identify why some pages are \"Crawled - Currently Not Indexed\" and optimize Googlebot's efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Environment & Dependency Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import socket\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from google.colab import auth\n",
    "import gspread\n",
    "from google.auth import default\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Constants\n",
    "LOGS_PATH = '/content/drive/MyDrive/Chargebee_Logs/'\n",
    "OUTPUT_SHEET_NAME = 'Chargebee_Audit_Findings'\n",
    "\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Data Ingestion (Memory Optimized)\n",
    "We process logs in chunks and filter for 'Googlebot' immediately to keep the memory footprint small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_logs(directory_path, chunk_size=50000):\n",
    "    all_files = glob.glob(os.path.join(directory_path, \"*.csv\"))\n",
    "    if not all_files:\n",
    "        print(f\"No CSV files found in {directory_path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    googlebot_data = []\n",
    "    \n",
    "    # Columns we actually need to save memory\n",
    "    use_cols = ['_time', 'useragent', 'uri_path', 'uri_query', 'status', 'bytes_sent', 'clientip', 'method']\n",
    "\n",
    "    for file in all_files:\n",
    "        print(f\"Processing {file}...\")\n",
    "        try:\n",
    "            # Read in chunks to avoid OOM\n",
    "            for chunk in pd.read_csv(file, usecols=use_cols, chunksize=chunk_size, low_memory=False):\n",
    "                # Filter for Googlebot (case-insensitive)\n",
    "                filtered_chunk = chunk[chunk['useragent'].str.contains(\"googlebot\", case=False, na=False)].copy()\n",
    "                googlebot_data.append(filtered_chunk)\n",
    "                \n",
    "                # Aggressive garbage collection\n",
    "                del chunk\n",
    "                gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "    if not googlebot_data:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.concat(googlebot_data, ignore_index=True)\n",
    "    print(f\"Ingestion complete. Total Googlebot rows: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "df_raw = ingest_logs(LOGS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Data Cleaning & IP Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_googlebot_ip(ip):\n",
    "    \"\"\"Performs Double-Reverse DNS lookup to verify Googlebot.\"\"\"\n",
    "    try:\n",
    "        host = socket.gethostbyaddr(ip)[0]\n",
    "        if not (host.endswith('.googlebot.com') or host.endswith('.google.com')):\n",
    "            return False\n",
    "        \n",
    "        addr = socket.gethostbyname(host)\n",
    "        return addr == ip\n",
    "    except (socket.herror, socket.gaierror):\n",
    "        return False\n",
    "\n",
    "def clean_data(df):\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # 1. Timestamp Conversion\n",
    "    df['_time'] = pd.to_datetime(df['_time'])\n",
    "    df.set_index('_time', inplace=True)\n",
    "    \n",
    "    # 2. IP Verification (Sample check if dataset is huge, or all unique IPs)\n",
    "    unique_ips = df['clientip'].unique()\n",
    "    print(f\"Verifying {len(unique_ips)} unique IPs...\")\n",
    "    ip_map = {ip: verify_googlebot_ip(ip) for ip in unique_ips}\n",
    "    df['is_verified_bot'] = df['clientip'].map(ip_map)\n",
    "    \n",
    "    # 3. File Type Categorization\n",
    "    def get_file_type(path):\n",
    "        if pd.isna(path): return 'Other'\n",
    "        ext = os.path.splitext(path)[1].lower()\n",
    "        if ext in ['', '.html', '.htm']: return 'HTML'\n",
    "        if ext == '.js': return 'JS'\n",
    "        if ext == '.css': return 'CSS'\n",
    "        if ext in ['.jpg', '.jpeg', '.png', '.gif', '.svg', '.webp']: return 'Image'\n",
    "        if ext in ['.json', '.xml']: return 'Data'\n",
    "        return 'Other'\n",
    "\n",
    "    df['file_type'] = df['uri_path'].apply(get_file_type)\n",
    "    \n",
    "    # 4. URL Normalization\n",
    "    df['full_url'] = df['uri_path'] + df['uri_query'].fillna('')\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = clean_data(df_raw)\n",
    "if not df.empty:\n",
    "    print(f\"Verified Bots: {df['is_verified_bot'].sum()} / {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Core Analysis Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module A: Crawl Volume & Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Hits\n",
    "daily_hits = df.resample('D').size()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "daily_hits.plot(title=\"Daily Googlebot Requests\")\n",
    "plt.ylabel(\"Number of Hits\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module B: Status Code Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_summary = df.groupby(['status', 'file_type']).size().unstack(fill_value=0)\n",
    "print(\"Status Code Distribution by File Type:\")\n",
    "display(status_summary)\n",
    "\n",
    "# Calculate 304 vs 200 for Resources\n",
    "resources = df[df['file_type'].isin(['JS', 'CSS'])]\n",
    "if not resources.empty:\n",
    "    cache_check = resources.groupby('status').size()\n",
    "    perc_304 = (cache_check.get(304, 0) / len(resources)) * 100\n",
    "    print(f\"\\nCache Control Check: {perc_304:.2f}% of JS/CSS requests returned 304 Not Modified.\")\n",
    "    if perc_304 < 90:\n",
    "        print(\"ALERT: Low 304 percentage suggests Cache Control Optimization Opportunity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module C: Spider Trap Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by clean path and count unique full URLs\n",
    "spider_traps = df.groupby('uri_path')['full_url'].nunique().sort_values(ascending=False).reset_index()\n",
    "spider_traps.columns = ['uri_path', 'unique_variations']\n",
    "\n",
    "top_traps = spider_traps[spider_traps['unique_variations'] > 50]\n",
    "print(\"Potential Spider Traps (> 50 variations):\")\n",
    "display(top_traps.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module D: Rendering Budget Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget_impact = df.groupby('file_type')['bytes_sent'].sum().reset_index()\n",
    "budget_impact['GB'] = budget_impact['bytes_sent'] / (1024**3)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(budget_impact['bytes_sent'], labels=budget_impact['file_type'], autopct='%1.1f%%')\n",
    "plt.title(\"Crawl Budget by Content Type (Bandwidth)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Additional Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Crawled Directories\n",
    "df['directory'] = df['uri_path'].apply(lambda x: '/' + x.split('/')[1] if pd.notna(x) and len(x.split('/')) > 1 else '/')\n",
    "top_dirs = df['directory'].value_counts().head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_dirs.values, y=top_dirs.index, palette=\"viridis\")\n",
    "plt.title(\"Top 10 Most Crawled Directories\")\n",
    "plt.xlabel(\"Hits\")\n",
    "plt.show()\n",
    "\n",
    "# Status Code Distribution\n",
    "status_counts = df['status'].value_counts().sort_index()\n",
    "colors = {200: 'green', 301: 'yellow', 302: 'yellow', 304: 'blue', 404: 'orange', 500: 'red', 503: 'red'}\n",
    "mapped_colors = [colors.get(x, 'grey') for x in status_counts.index]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "status_counts.plot(kind='bar', color=mapped_colors)\n",
    "plt.title(\"Status Code Distribution\")\n",
    "plt.xlabel(\"Status Code\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Export to Google Sheets\n",
    "Publishes the findings to a Google Sheet for reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_sheets(spider_traps_df, status_df):\n",
    "    auth.authenticate_user()\n",
    "    creds, _ = default()\n",
    "    gc_sheet = gspread.authorize(creds)\n",
    "\n",
    "    try:\n",
    "        sh = gc_sheet.open(OUTPUT_SHEET_NAME)\n",
    "    except gspread.SpreadsheetNotFound:\n",
    "        sh = gc_sheet.create(OUTPUT_SHEET_NAME)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%m/%d %H:%M\")\n",
    "    \n",
    "    # Export Spider Traps\n",
    "    ws_trap = sh.add_worksheet(title=f\"Spider Traps {timestamp}\", rows=len(spider_traps_df) + 10, cols=20)\n",
    "    ws_trap.update('A1', [spider_traps_df.columns.values.tolist()] + spider_traps_df.fillna('').values.tolist())\n",
    "    \n",
    "    # Export Status Summary\n",
    "    ws_status = sh.add_worksheet(title=f\"Status Summary {timestamp}\", rows=len(status_df) + 10, cols=20)\n",
    "    # Flatten the status_df for export\n",
    "    status_flat = status_df.reset_index()\n",
    "    ws_status.update('A1', [status_flat.columns.values.tolist()] + status_flat.fillna('').values.tolist())\n",
    "    \n",
    "    print(f\"Results exported successfully! URL: {sh.url}\")\n",
    "\n",
    "export_to_sheets(top_traps, status_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
