# Technical Guide: Chargebee Log Analysis Engine

This guide provides an exhaustive technical breakdown of the `Chargebee_Log_Analysis.ipynb` notebook. It is designed for technical stakeholders to understand exactly how every datapoint is generated, the underlying Python/Pandas logic, and the organizational buckets used for segmentation.

---

## 1. High-Level Pipeline Overview

The analysis follows a sequential data science pipeline:
1.  **Ingestion**: Memory-optimized chunked reading of Splunk-exported CSV logs.
2.  **Preprocessing**: Timestamp normalization, Double-Reverse DNS (drDNS) bot verification, and attribute extraction.
3.  **Segmentation**: Organizational bucketing by file type, bot type, and directory.
4.  **Analysis**: Statistical calculation of crawl efficiency, caching health, and performance.
5.  **Reporting**: Exporting to Google Sheets with automated summary statistics and SD band calculations.

---

## 2. Datapoint Dictionary & Generation Logic

This section details the specific logic behind every calculated datapoint in the engine.

### A. Preprocessing & Cleaning
*   **Verified Bot (`is_verified_bot`)**:
    *   **Logic**: Uses Double-Reverse DNS (drDNS). It takes the `clientip`, performs a reverse lookup (`socket.gethostbyaddr`) to check if the hostname ends in `.googlebot.com`, `.google.com`, or `.googleusercontent.com`. Then, it performs a forward lookup (`socket.gethostbyname`) on that hostname. If the resulting IP matches the original `clientip`, the bot is verified.
    *   **Python Snippet**: `socket.gethostbyaddr(ip)` -> `socket.gethostbyname(host) == ip`.
*   **Full URL (`full_url`)**:
    *   **Logic**: Concatenates `uri_path` and `uri_query` to create the complete unique resource identifier seen by the bot.
    *   **Pandas Snippet**: `df['uri_path'] + df['uri_query'].fillna('')`.
*   **Directory (`directory`)**:
    *   **Logic**: Truncates the `uri_path` to a maximum depth of 2 (e.g., `/resources/glossaries/page` becomes `/resources/glossaries`). This groups pages into logical SEO silos.
    *   **Python Snippet**: `'/'.join(path.split('/')[:3])`.

### B. Organizational Buckets ("Other" and "Data")
The engine throws resources and bots into buckets to simplify high-level analysis.

#### 1. File Type Buckets
Generated via `os.path.splitext(path)[1].lower()`.
*   **Data**: Contains structured data formats used for API or dynamic content.
    *   **Extensions**: `.json`, `.xml`.
*   **Other**: A "catch-all" for resources that don't fit into HTML, JS, CSS, or Image categories.
    *   **Includes**: `NaN` (missing) paths.
    *   **Common Examples**: Technical assets or documents such as `.txt`, `.pdf`, `.woff`, `.woff2`, `.ttf`, `.eot`, `.mp4`, `.ico`, and `.map`.

#### 2. Bot Type Buckets
Generated by regex matching against the `useragent` string.
*   **Smartphone**: `Googlebot` + `Mobile|Android|iPhone`.
*   **Desktop**: `Googlebot` without mobile indicators.
*   **Specialized Bots**: `Image`, `Video`, `News`, `StoreBot`, `AdsBot`, `AdSense`.
*   **Other Google/Unknown**:
    *   **Logic**: Any User-Agent that contains the string "google" (captured during ingestion) but fails to match any of the specific patterns above. This often includes legacy bots or miscellaneous Google services.

---

## 3. Specialized Metrics Deep Dive

### A. Relative Crawl Frequency
*   **Purpose**: To identify which site sections are "over-crawled" or "under-crawled" relative to their size.
*   **Logic**: Divide the total number of hits to a directory by the number of unique URLs found within that directory.
*   **Formula**: `Total Hits / Unique URLs`.
*   **Interpretation**: A high number means Googlebot is returning to the same pages frequently. A number close to 1.0 means Googlebot is mostly discovering "new" pages and rarely returning.

### B. Potential Hits Eliminated (Theoretical Savings)
*   **Purpose**: To quantify the bandwidth and crawl budget wasted by not serving `304 Not Modified` headers for static/unchanged content.
*   **Logic**: For every unique URL, we assume the first two `200 OK` hits are necessary (initial discovery and one validation). Every subsequent `200 OK` is treated as a hit that *could* have been a `304 Not Modified` if caching headers were perfectly implemented.
*   **Formula**: `Sum(Max(0, Count of 200 OK responses - 2))` across all unique URLs.
*   **Pandas Snippet**: `ok_hits.groupby('full_url').size().apply(lambda x: max(0, x - 2)).sum()`.

### C. Standard Deviation (SD) Bands
*   **Purpose**: To provide context to performance and volume metrics by identifying outliers.
*   **Logic**: Metrics are compared against site-wide "Global Benchmarks" (means and standard deviations). Counts are then aggregated into five bands:
    *   **Above +2SD**: Extreme high outliers.
    *   **+1SD to +2SD**: Significantly above average.
    *   **-1SD to +1SD**: Normal range (the "middle" ~68% of data).
    *   **-2SD to -1SD**: Significantly below average.
    *   **Below -2SD**: Extreme low outliers.
*   **Benchmark Principle**: Per user requirements, these bands always use site-wide globals as the baseline to ensure that "Above +2SD" means the same thing across different reports.

---

## 4. Analysis Module Logic

### Module C: Spider Trap Detection
*   **Variations**: The number of unique `full_url` values associated with a single `uri_path`.
*   **Bloat Params**: Extracted by parsing the query strings of all variations for a path and counting the frequency of individual keys using `urllib.parse.parse_qs`.
*   **Trap Identification**: Any `uri_path` with `Variations > 50` is flagged as a potential trap.

### Module E: Performance & Latency
*   **Avg Latency**: The arithmetic mean of the `time_taken` field (measured in milliseconds) per directory.
*   **Total Bandwidth**: The sum of `bytes_sent` (converted to Megabytes) per directory.
*   **Concentration of Errors**: The sum of boolean flags where `status >= 400`.

---

## 5. Justifications: Design Decisions

1.  **HTML-Only Directory Analysis**: When calculating directory-level SEO metrics (latency, bandwidth, crawl volume), we exclude static assets (JS, CSS, Images). This ensures that a folder appearing "slow" is due to HTML rendering issues, not just because it contains many images.
2.  **Horizontal Sheets Layout**: Data is exported horizontally with summary statistics adjacent to main tables. This allows for side-by-side comparison of raw data and its statistical distribution.
3.  **Chunked Ingestion**: Files are read in 50,000-row chunks to prevent Google Colab OOM (Out of Memory) crashes, a necessity given the multi-gigabyte size of annual server logs.
